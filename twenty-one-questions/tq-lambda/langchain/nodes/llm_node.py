"""
Node that uses the OpenAI client to call the LLM for the next question.
"""

from llm import OpenAIClient
import sys
import os
from typing import List, Dict, Optional

# Add the root project directory to the path to import the llm module
# This assumes the lambda is deployed with the llm module available
# Try multiple possible paths for Lambda deployment
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../../../"))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Also try the current directory structure (if llm is packaged with lambda)
lambda_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
if lambda_root not in sys.path:
    sys.path.insert(0, lambda_root)


def get_next_question(
    questions_and_answers: Optional[List[Dict[str, str]]] = None,
    current_question_number: int = 1,
    model: str = "gpt-4",
    api_key: Optional[str] = None,
    **kwargs
) -> str:
    """
    Use the OpenAI client to get the next question for the 21 questions game.

    Args:
        questions_and_answers: List of previous Q&A pairs. Each dict should have:
            - 'question': The question asked
            - 'answer': The answer given
        current_question_number: The current question number (1-21)
        model: The OpenAI model to use. Defaults to "gpt-4".
        api_key: Optional API key. If not provided, uses OPENAI_API_KEY env var.
        **kwargs: Additional arguments to pass to the API (e.g., temperature, max_tokens).

    Returns:
        str: The next question generated by the LLM
    """
    from .prompt_node import create_21_questions_prompt

    # Create the prompt
    prompt = create_21_questions_prompt(questions_and_answers, current_question_number)

    # Initialize the OpenAI client
    client = OpenAIClient(api_key=api_key)

    # Call the LLM
    response = client.call(prompt=prompt, model=model, temperature=0.7, max_tokens=200, **kwargs)

    # Extract the question from the response
    question = response.choices[0].message.content.strip()

    return question
